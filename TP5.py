# -*- coding: utf-8 -*-
"""Copie de TP 5 Emma.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uekWP9Z66HiKOYrbHU1fGbvU67pZNx5-
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Colab Notebooks

!pwd

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.14

import tensorflow
print(tensorflow.__version__)

import matplotlib as mlp
import matplotlib.pyplot as plt
import numpy as np
import tkinter as tk
import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Activation
import pandas as pd
import _pickle as pickle
import matplotlib.image as mpimg

"""**TP 5 - Vision et language**

Exercice 1 : Simplification du vocabulaire

On extrait un histogramme d’occurrence des mots présents dans les légendes du sous-ensemble d’apprentissage de la base Flickr8k
"""

filename = 'flickr_8k_train_dataset.txt'
df = pd.read_csv(filename, delimiter='\t')
nb_samples = df.shape[0]
iter = df.iterrows()

bow = {}
nbwords = 0

for i in range(nb_samples):
 x = iter.__next__()
 cap_words = x[1][1].split() # split caption into words
 cap_wordsl = [w.lower() for w in cap_words] # remove capital letters
 nbwords += len(cap_wordsl)
 for w in cap_wordsl:
  if (w in bow):
   bow[w] = bow[w]+1
  else:
   bow[w] = 1

bown = sorted([(value,key) for (key,value) in bow.items()], reverse=True)

"""On va charger le fichier d’embedding utilisé au TP précédent (que l’on peut récupérer ici : http://cedric.cnam.fr/~thomen/cours/US330X/Caption_Embeddings.p), et conserver les nbkeep mots les plus fréquents, et sauvegarder le sous-ensemble de mots et les embeddings vectoriels Glove correspondants"""

nbkeep = 1000 # 100 is needed for fast processing

outfile = 'Caption_Embeddings.p'
[listwords, embeddings] = pickle.load( open( outfile, "rb" ) )

embeddings_new = np.zeros((nbkeep,102))
listwords_new = []

for i in range(nbkeep):
 listwords_new.append(bown[i][1])
 embeddings_new[i,:] = embeddings[i]# COMPLETE WITH YOUR CODE
 embeddings_new[i,:] /= np.linalg.norm(embeddings_new[i,:]) # Normalization


listwords = listwords_new
embeddings = embeddings_new
outfile = "Caption_Embeddings_"+str(nbkeep)+".p"
with open(outfile, "wb" ) as pickle_f:
 pickle.dump( [listwords, embeddings], pickle_f)

"""On pourra calculer la fréquence cumulée des 100 premiers mots conservés, et l’afficher de la manière suivante :


"""

freqnc = np.cumsum([float(w[0])/nbwords*100.0 for w in bown])

x_axis = [str(bown[i][1]) for i in range(100)]
plt.figure(dpi=300)
plt.xticks(rotation=90, fontsize=3)
plt.ylabel('Word Frequency')
plt.bar(x_axis, freqnc[0:100])

print("number of kept words="+str(nbkeep)+" - ratio="+str(freqnc[nbkeep-1])+" %")

"""```
# Ce texte est au format code
```

**Exercice 2 : Création des données d’apprentissage et de test**

Ns = 30000(60000 image 5 legendes par images)  Ls=35 (nombre max de mots dans toutes les légendes car on a besoin de tenseurs de taille fixe, il faut que ce soit celle de la phrase maximale) d=202(taille du veteur d’embedding 100 pour embedding visuel (VGG) 100 pour le glove et 2 pour start et end)
 On ne génère des légendes qu’avec les 1000mots de vocabulaire que l’on a conservés.

Comme on doit utiliser des tenseurs de taille fixe avec keras, on va déterminer la longueur de la légende maximale dans les données d’entraînement :
"""

filename = 'flickr_8k_train_dataset.txt'
df = pd.read_csv(filename, delimiter='\t')
nbTrain = df.shape[0]
iter = df.iterrows()

caps = [] # Set of captions
imgs = [] # Set of images
for i in range(nbTrain):
 x = iter.__next__()
 caps.append(x[1][1])
 imgs.append(x[1][0])

maxLCap = 0

for caption in caps:
   l=0
   words_in_caption =  caption.split()
   for j in range(len(words_in_caption)-1):
    current_w = words_in_caption[j].lower()
    if(current_w in listwords):
     l+=1
   if(l > maxLCap):
       maxLCap = l
print("max caption length ="+str(maxLCap))

"""Construction des tenseurs des données et des labels :

Récupération du vecteur contenant le features des images
"""

outfile = "Caption_Embeddings_"+str(nbkeep)+".p"
[listwords, embeddings] = pickle.load( open( outfile, "rb" ) ) # Loading reduced dictionary
indexwords = {} # Useful for tensor filling
for i in range(len(listwords)):
 indexwords[listwords[i]] = i

# Loading images features
encoded_images = pickle.load( open( "encoded_images_PCA.p", "rb" ) )

# Allocating data and labels tensors
tinput = 202
tVocabulary = len(listwords)
X_train = np.zeros((nbTrain,maxLCap, tinput))
Y_train = np.zeros((nbTrain,maxLCap, tVocabulary), bool)

for i in range(nbTrain):
 words_in_caption =  caps[i].split()
 indseq=0 # current sequence index (to handle mising words in reduced dictionary)
 for j in range(len(words_in_caption)-1):
  current_w = words_in_caption[j].lower()
  if(current_w in listwords):
   X_train[i,indseq,0:100] = encoded_images[imgs[i]] # COMPLETE WITH YOUR CODE
   X_train[i,indseq,100:202] = embeddings[indexwords[current_w]]# COMPLETE WITH YOUR CODE

  next_w = words_in_caption[j+1].lower()
  if(next_w in listwords):
   index_pred = indexwords[next_w]# COMPLETE WITH YOUR CODE
   Y_train[i,indseq,index_pred] =1 # COMPLETE WITH YOUR CODE
   indseq += 1 # Increment index if target label present in reduced dictionary

outfile = 'Training_data_'+str(nbkeep)
np.savez(outfile, X_train=X_train, Y_train=Y_train) # Saving tensor

"""On fera la même chose pour les données de test, en allouant des tenseurs de la même taille que ceux d’entraînement (et pouvoir ainsi appliquer le modèle de prédiction ensuite)."""

filename = 'flickr_8k_test_dataset.txt'
df = pd.read_csv(filename, delimiter='\t')
nbTest = df.shape[0]
iter = df.iterrows()

caps = [] # Set of captions
imgs = [] # Set of images
for i in range(nbTest):
 x = iter.__next__()
 caps.append(x[1][1])
 imgs.append(x[1][0])

maxLCap = 0

for caption in caps:
   l=0
   words_in_caption =  caption.split()
   for j in range(len(words_in_caption)-1):
    current_w = words_in_caption[j].lower()
    if(current_w in listwords):
     l+=1
   if(l > maxLCap):
       maxLCap = l
print("max caption length ="+str(maxLCap))

outfile = "Caption_Embeddings_"+str(nbkeep)+".p"
[listwords, embeddings] = pickle.load( open( outfile, "rb" ) ) # Loading reduced dictionary
indexwords = {} # Useful for tensor filling
for i in range(len(listwords)):
 indexwords[listwords[i]] = i

# Loading images features
encoded_images = pickle.load( open( "encoded_images_PCA.p", "rb" ) )

# Allocating data and labels tensors
tinput = 202
tVocabulary = len(listwords)
X_test = np.zeros((nbTest,maxLCap, tinput))
Y_test = np.zeros((nbTest,maxLCap, tVocabulary), bool)

for i in range(nbTest):
 words_in_caption =  caps[i].split()
 indseq=0 # current sequence index (to handle mising words in reduced dictionary)
 for j in range(len(words_in_caption)-1):
  current_w = words_in_caption[j].lower()
  if(current_w in listwords):
   X_test[i,indseq,0:100] = encoded_images[imgs[i]] # COMPLETE WITH YOUR CODE
   X_test[i,indseq,100:202] = embeddings[indexwords[current_w]]# COMPLETE WITH YOUR CODE

  next_w = words_in_caption[j+1].lower()
  if(next_w in listwords):
   index_pred = indexwords[next_w]# COMPLETE WITH YOUR CODE
   Y_test[i,indseq,index_pred] =1 # COMPLETE WITH YOUR CODE
   indseq += 1 # Increment index if target label present in reduced dictionary

outfile = 'Testing_data_'+str(nbkeep)
np.savez(outfile, X_test=X_test, Y_test=Y_test) # Saving tensor

"""**Exercice 3 : Entraînement du modèle**"""

import keras
from keras.layers.recurrent import SimpleRNN
from keras.models import Sequential
from keras.layers import Dense, Activation
from keras.optimizers import SGD, RMSprop

# On pourra utiliser la méthode saveModel pour stocker le modèle appris :
from keras.models import model_from_yaml
def saveModel(model, savename):
  # serialize model to YAML
  model_yaml = model.to_yaml()
  with open(savename+".yaml", "w") as yaml_file:
    yaml_file.write(model_yaml)
    print("Yaml Model ",savename,".yaml saved to disk")
  # serialize weights to HDF5
  model.save_weights(savename+".h5")
  print("Weights ",savename,".h5 saved to disk")

model = Sequential()

samples, timesteps, features = 30000, 35, 202

model.add(keras.layers.Masking(mask_value=0.,input_shape=(timesteps, features)))

model.add(keras.layers.SimpleRNN(100, return_sequences=True, unroll=True))
model.add(keras.layers.Dense(1000, activation='softmax'))

model.summary()

model.compile(optimizer='Adam', loss="categorical_crossentropy",metrics=['accuracy'])

model.fit(x=X_train, y=Y_train, batch_size=10, epochs=30)

saveModel(model, 'model')

"""**Exercice 4 : Évaluation du modèle**"""

def loadModel(savename):
 with open(savename+".yaml", "r") as yaml_file:
  model = model_from_yaml(yaml_file.read())
 print ("Yaml Model ",savename,".yaml loaded ")
 model.load_weights(savename+".h5")
 print ("Weights ",savename,".h5 loaded ")
 return model

# LOADING MODEL
nameModel = 'model' # COMPLETE with your model name
model = loadModel(nameModel)

optim = keras.optimizers.Adam()
model.compile(loss="categorical_crossentropy", optimizer=optim,metrics=['accuracy'])

# LOADING TEST DATA
outfile = 'Testing_data_'+str(nbkeep)+'.npz'
npzfile = np.load(outfile)

X_test = npzfile['X_test']
Y_test = npzfile['Y_test']

outfile = "Caption_Embeddings_"+str(nbkeep)+".p"
[listwords, embeddings] = pickle.load( open( outfile, "rb" ) )
indexwords = {}
for i in range(len(listwords)):
 indexwords[listwords[i]] = i

"""On va ensuite sélectionner une image parmi l’ensemble de test, l’afficher, ainsi qu’une légende issues des annotations."""

ind = np.random.randint(X_test.shape[0])

filename = 'flickr_8k_test_dataset.txt' #  PATH IF NEEDED

df = pd.read_csv(filename, delimiter='\t')
iter = df.iterrows()

for i in range(ind+1):
 x = iter.__next__()

imname = x[1][0]
print("image name="+imname+"caption="+x[1][1])
#dirIm = "/content/Flicker8k_Dataset/" # CHANGE WITH YOUR DATASET
dirIm = "Flicker8k_Dataset/" # CHANGE WITH YOUR DATASET

img=mpimg.imread(dirIm+imname)
plt.figure(dpi=100)
plt.imshow(img)
plt.axis('off')
plt.show()

##à télécharger

"""Prédiction"""

pred = model.predict(X_test[ind:ind+1,:,:])

def sampling(preds, temperature=1.0):
 preds = np.asarray(preds).astype('float64')
 predsN = pow(preds,1.0/temperature)
 predsN /= np.sum(predsN)
 probas = np.random.multinomial(1, predsN, 1)
 return np.argmax(probas)

nbGen = 5
temperature=0.1 # Temperature param for peacking soft-max distribution

for s in range(nbGen):
 wordpreds = "Caption n° "+str(s+1)+": "
 indpred = sampling(pred[0,0,:], temperature)
 wordpred = listwords[indpred]
 wordpreds +=str(wordpred)+ " "
 X_test[ind:ind+1,1,100:202] =embeddings[indpred,:] # COMPLETE WITH YOUR CODE
 cpt=1
 while(str(wordpred)!='<end>' and cpt<30):
  pred = model.predict(X_test[ind:ind+1,:,:])
  indpred = sampling(pred[0,cpt,:], temperature)
  wordpred = listwords[indpred]
  wordpreds += str(wordpred)+ " "
  cpt+=1
  X_test[ind:ind+1,cpt,100:202] =embeddings[indpred,:] # COMPLETE WITH YOUR CODE

print(wordpreds)