# -*- coding: utf-8 -*-
"""TP4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1neibgmPkR7ELfjivfimHIRon-zAMH1KU
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.14

import tensorflow
print(tensorflow.__version__)

import matplotlib as mlp
import matplotlib.pyplot as plt
import numpy as np
import tkinter as tk
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Activation

"""**Exercice 1 - Génération de poésie**"""

bStart = False
fin = open("fleurs_mal.txt", 'r' , encoding = 'utf8')
lines = fin.readlines()
lines2 = []
text = []

for line in lines:
 line = line.strip().lower() # Remove blanks and capitals
 if("Charles Baudelaire avait un ami".lower() in line and bStart==False):
  print("START")
  bStart = True
 if("End of the Project Gutenberg EBook of Les Fleurs du Mal, by Charles Baudelaire".lower() in line):
  print("END")
  break
 if(bStart==False or len(line) == 0):
  continue
 lines2.append(line)

fin.close()
text = " ".join(lines2)
chars = sorted(set([c for c in text]))
nb_chars = len(chars)

"""Comment s’interprète la variable chars ? Que représente nb_chars ?
chars est le dictionnaire 
il y en a 60 éléments
"""

SEQLEN = 10 # Length of the sequence to predict next char
STEP = 1 # stride between two subsequent sequences
input_chars = []
label_chars = []
for i in range(0, len(text) - SEQLEN, STEP):
       input_chars.append(text[i:i + SEQLEN])
       label_chars.append(text[i + SEQLEN])
nbex = len(input_chars)

"""A partir d'un vecteur d'entrée on veut prédir le caractère suivant, on costruit artificiellemnt un pb de classification 

Vectorisation des données d’entraînement en utilisant le dictionnaire et un encodage one-hot pour chaque caractère
"""

# mapping char -> index in dictionary: used for encoding (here)
char2index = dict((c, i) for i, c in enumerate(chars))
# mapping char -> index in dictionary: used for decoding, i.e. generation - part c)
index2char = dict((i, c) for i, c in enumerate(chars)) # mapping index -> char in dictionary

"""* **Question** :

Compléter le code suivant pour créer les données et labels d’entraînement. N.B. : utiliser la variable char2index.* 
"""

import numpy as np
X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)
y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)

for i, input_char in enumerate(input_chars):
 for j, ch in enumerate(input_char):
    # Fill X at correct index
     l = char2index.get(ch)
     #print(l)
     X[i,j,l] = True;
     # Fill y at correct index
     l = char2index[label_chars[i]]
     y[i,l] = True;

"""Séparation des données en 2 ensembles d'apprentissage et de test, puis sauvegarde"""

import _pickle as pickle

ratio_train = 0.8
nb_train = int(round(len(input_chars)*ratio_train))
print("nb tot=",len(input_chars) , "nb_train=",nb_train)
X_train = X[0:nb_train,:,:]
y_train = y[0:nb_train,:]

X_test = X[nb_train:,:,:]
y_test = y[nb_train:,:]
print("X train.shape=",X_train.shape)
print("y train.shape=",y_train.shape)

print("X test.shape=",X_test.shape)
print("y test.shape=",y_test.shape)

outfile = "Baudelaire_len_"+str(SEQLEN)+".p"

with open(outfile, "wb" ) as pickle_f:
 pickle.dump( [index2char, X_train, y_train, X_test, y_test], pickle_f)

"""**b)  Apprentissage d’un modèle auto-supervisé pour la génération de texte**

Chargement des données
"""

SEQLEN = 10
outfile = "Baudelaire_len_"+str(SEQLEN)+".p"
[index2char, X_train, y_train, X_test, y_test] = pickle.load( open( outfile, "rb" ) )

"""On a ici un pb de many to one, on se sert de l'historique des 10 présédents pour avoir le suivant

création d'un modèle séquentiel
"""

from keras.layers.recurrent import SimpleRNN
from keras.models import Sequential
from keras.layers import Dense, Activation
from keras.optimizers import SGD, RMSprop

model = Sequential()

"""Ajout d'une couche récurrente avec un modèle de type SimpleRNN """

HSIZE = 128
model.add(SimpleRNN(HSIZE, return_sequences=False, input_shape=(SEQLEN, nb_chars),unroll=True))

"""**Question** :

Expliquer à quoi correspond return_sequences=False.
return_sequences=False indique que l'on ne souhaite pas renvoyer la dernière sortie de la séquence de sortie ou la séquence complète

On ajoutera enfin une couche complètement connectée suivie d’une fonction softmax pour effectuer la classification du caractère suivant la séquence.
"""

model.add(Dense(nb_chars))
model.add(Activation("softmax"))

"""Pour optimiser des réseaux récurrents, on utilise préférentiellement des méthodes adaptatives comme RMSprop. On pourra donc compiler le modèle et utiliser la méthode summary() pour visualiser le nombre de paramètres du réseaux"""

BATCH_SIZE = 128
NUM_EPOCHS = 50
learning_rate = 0.001
optim = RMSprop(lr=learning_rate)
model.compile(loss="categorical_crossentropy", optimizer=optim,metrics=['accuracy'])
model.summary()

"""L’entraînement sera effectué comme habituellement avec la méthode fit():"""

model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS)

scores_train = model.evaluate(X_train, y_train, verbose=1)
scores_test = model.evaluate(X_test, y_test, verbose=1)
print("PERFS TRAIN: %s: %.2f%%" % (model.metrics_names[1], scores_train[1]*100))
print("PERFS TEST: %s: %.2f%%" % (model.metrics_names[1], scores_test[1]*100))

"""On pourra utiliser la méthode saveModel pour stocker le modèle appris :"""

# On pourra utiliser la méthode saveModel pour stocker le modèle appris :
from keras.models import model_from_yaml
def saveModel(model, savename):
  # serialize model to YAML
  model_yaml = model.to_yaml()
  with open(savename+".yaml", "w") as yaml_file:
    yaml_file.write(model_yaml)
    print("Yaml Model ",savename,".yaml saved to disk")
  # serialize weights to HDF5
  model.save_weights(savename+".h5")
  print("Weights ",savename,".h5 saved to disk")

saveModel(model,'model_recu')

"""**Analyse de l’apprentissage**
Quels taux de classification obtient-on en apprentissage ? Nous avons obtenu un taux d'apprentissage de 53,67%

Commenter les performances obtenues.

Nous voyons que le modèles n'est pas très performant (faible taux d'apprentissage 53% cette performance diminue avec le test et on se retrouve avec un taux de performance de 46,82%

En quoi le problème est-il différents des problèmes de classification abordés jusqu’ici ?

Au niveau des autres problèmes que nous avons eu à voir, il n'yavait pas d'ordre entre les éléments des données d'entrée. Par contre dans ce modèle, nous avons non seulement une chronologie entre les éléments des données d'entrée, mais aussi la prédiction d'un élément dépend de la prédiction de lélément qui l'a précédé.

**c) Génération de texte avec le modèle appris**
"""

SEQLEN = 10
outfile = "Baudelaire_len_"+str(SEQLEN)+".p"
[index2char, X_train, y_train, X_test, y_test] = pickle.load( open( outfile, "rb" ) )

"""Et le réseau récurrent avec la fonction loadModel :"""

# Chargement du réseau récurrent
from keras.models import model_from_yaml
def loadModel(savename): # Définition de la fonction
  with open(savename+".yaml", "r") as yaml_file:
    model = model_from_yaml(yaml_file.read())
  print("Yaml Model ",savename,".yaml loaded ")
  model.load_weights(savename+".h5")
  print("Weights ",savename,".h5 loaded ")
  return model

"""On pourra vérifier l’architecture du réseau avec la méthode summary:"""

# Chargement du modèle
loadModel('model_recu')
model.compile(loss='categorical_crossentropy',optimizer='RMSprop',metrics=['accuracy'])
model.summary()
nb_chars = len(index2char)

"""On va maintenant sélectionner une chaîne de caractère initiale pour notre réseau, afin de prédire le caractère suivant :"""

seed =15608
char_init = ""
for i in range(SEQLEN):
 char = index2char[np.argmax(X_train[seed,i,:])]
 char_init += char

print("CHAR INIT: "+char_init)

"""On va maintenant convertir la séquence de départ au format one-hot pour appliquer le modèle de prédiction."""

test = np.zeros((1, SEQLEN, nb_chars), dtype=np.bool)
test[0,:,:] = X_train[seed,:,:]

"""Au lieu de prédire directement la sortie de probabilité maximale, on va échantillonner une sortie tirée selon la distribution de probabilités du soft-max. Pour commencer on va utiliser un paramètre de température pour rendre la distribution plus ou moins piquée.

Echantillonage après transformation de distribution
"""

def sampling(preds, temperature=1.0):
 preds = np.asarray(preds).astype('float64')
 predsN = pow(preds,1.0/temperature)
 predsN /= np.sum(predsN)
 probas = np.random.multinomial(1, predsN, 1)
 return np.argmax(probas)

"""**Questions**

Quel va être le comportement de cet échantillonnage lorsque la température T augmente (T→+∞) ou diminue (T→0) ?

lorsque (T→0) on se rapproche d'une distribution où l'on nous retourne toujours la valeur maximale, cela génère donc toujours à peu près la mmême chose, on obtient alors quelque chose de très répétitif 

lorsque (T→+∞) on tend vers une distribution uniforme, ce qui n'a pas d'intéret puisque cela nous génère quelque chose d'aléatoire 

Il faut un entre deux, pour obtenir un texte convenable

On va maintenant mettre en place la génération de texte à partir d’une séquence de SEQLEN caractère initiaux. On pourra utiliser le code suivant :
"""

nbgen = 400 # number of characters to generate (1,nb_chars)
gen_char = char_init
temperature  = 0.5

for i in range(nbgen):
 preds = model.predict(test)[0]  # shape (1,nb_chars)
 next_ind = sampling(preds,temperature)
 next_char = index2char[next_ind]
 gen_char += next_char
 for i in range(SEQLEN-1):
  test[0,i,:] = test[0,i+1,:]
 test[0,SEQLEN-1,:] = 0
 test[0,SEQLEN-1,next_ind] = 1

print("Generated text: "+gen_char)

"""**Analyse de la génération**

Evaluer l’impact du paramètre de température dans la génération, ainsi que le nombre d’époques dans l’apprentissage. Commenter les points forts et points faibles du générateur.

C'est assez remarquable la génération de texte est sémantiquement correcte

**Exercice 2 : Embedding Vectoriel de texte**

On clusterise les mots en 10 clusters, on veut que les clusters regroupent des mots sémantiquement proches.

**a) Extraction des embedding Glove des légendes**
"""

import pandas as pd
filename = 'flickr_8k_train_dataset.txt'
df = pd.read_csv(filename, delimiter='\t')
nb_samples = df.shape[0]
iter = df.iterrows()
allwords = []
for i in range(nb_samples):
 x = iter.__next__()
 cap_words = x[1][1].split() # split caption into words
 cap_wordsl = [w.lower() for w in cap_words] # remove capital letters
 allwords.extend(cap_wordsl)

unique = list(set(allwords)) # List of different words in captions
print(len(unique))

"""On va maintenant télécharger le fichier contenant les Embeddings vectoriels Glove : http://cedric.cnam.fr/~thomen/cours/US330X/glove.6B.100d.txt :"""

GLOVE_MODEL = "glove.6B.100d.txt"
fglove = open(GLOVE_MODEL, "r")

"""On maintenant déterminer la liste des mots présents dans les légendes et dans le fichier Glove."""

import numpy as np
cpt=0
listwords=[]
listembeddings=[]
for line in fglove:
 row = line.strip().split()
 word = row[0]
 if(word in unique or word=='unk'):
  listwords.append(word)
  embedding = np.array(row[1:],dtype="float32")
  listembeddings.append(embedding)
  cpt +=1
  #print("word: "+word+" embedded "+str(cpt))

fglove.close()
nbwords = len(listembeddings)
tembedding = len(listembeddings[0])
print("Number of words="+str(len(listembeddings))+" Embedding size="+str(tembedding))

"""N.B. : on a ajouté le mot “unk” qui est destiné à coder les mots des légendes absents du fichiers d’embedding.

On va finalement créer la matrice des embedding, en ajoutant deux mots pour coder les mots “<start>” et “<end>” (utile pour le TP suivant) :
"""

embeddings = np.zeros((len(listembeddings)+2,tembedding+2))
for i in range(nbwords):
 embeddings[i,0:tembedding] = listembeddings[i]

listwords.append('<start>')
embeddings[7001,100] = 1
listwords.append('<end>')
embeddings[7002,101] = 1

#sauvegarder la liste des mots et les vecteurs associés :
import _pickle as pickle

outfile = 'Caption_Embeddings.p'
with open(outfile, "wb" ) as pickle_f:
 pickle.dump( [listwords, embeddings], pickle_f)

"""**b) Analyse des embedding Glove des légendes**

On va commencer par ouvrir le fichier des embeddings, puis à normaliser les vecteurs pour qu’ils aient une norme euclidienne unité :
"""

import numpy as np
import _pickle as pickle

outfile = 'Caption_Embeddings.p'
[listwords, embeddings] = pickle.load( open( outfile, "rb" ) )
print("embeddings: "+str(embeddings.shape))

for i in range(embeddings.shape[0]):
 embeddings[i,:] /= np.linalg.norm(embeddings[i,:])

"""**Question** :

Expliquer l’objectif de la normalisation

On va maintenant effectuer un clustering dans l’espace des embeddings en 10 groupes avec l’algorithme du KMeans : https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html. On utilisera max_iter=1000 et init=”random”.
"""

from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=10, max_iter=1000, init="random").fit(embeddings) # COMPLETE WITH YOUR CODE - apply fit() method on embeddings
clustersID  = kmeans.labels_
clusters = kmeans.cluster_centers_

"""Afin d’afficher les points le point le plus proche de chaque centre, ainsi que les 20 points suivants les plus proche du centre, on pourra utiliser le code suivant :"""

from sklearn.manifold import TSNE
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
indclusters=np.ndarray((10,7003),dtype=int)
for i in range(10):
 norm = np.linalg.norm((clusters[i] - embeddings),axis=1)
 inorms = np.argsort(norm)
 indclusters[i][:] = inorms[:]

 print("Cluster "+str(i)+" ="+listwords[indclusters[i][0]])
 for j in range(1,21):
  print(" mot: "+listwords[indclusters[i][j]])

"""**Question** :

Montrer le résultat des centre du clustering obtenu ainsi que les plus proches de chaque centre. Commenter le résultat par rapport à la sémantique des mots.

On remarque que les 20 mots les plus proches du centre, sont de sémantique proche.
"""

tsne = TSNE(n_components=2, perplexity=30, verbose=2, init='pca', early_exaggeration=24)
points2D = tsne.fit_transform(embeddings)

"""Pour visualiser la répartition des points dans l’espace d’embedding, on pourra utiliser la méthode t-SNE

Et afficher les points des différents clusters ainsi que le centre avec un croix ainsi :
"""

pointsclusters = np.ndarray((10,2),dtype=int)
for i in range(10):
 pointsclusters[i,:] = points2D[int(indclusters[i][0])]

cmap =cm.tab10
plt.figure(figsize=(3.841, 7.195), dpi=100)
plt.set_cmap(cmap)
plt.subplots_adjust(hspace=0.4 )
plt.scatter(points2D[:,0], points2D[:,1], c=clustersID,  s=3,edgecolors='none', cmap=cmap, alpha=1.0)
plt.scatter(pointsclusters[:,0], pointsclusters[:,1], c=range(10),marker = '+', s=1000, edgecolors='none', cmap=cmap, alpha=1.0)

plt.colorbar(ticks=range(10))
plt.show()

"""On observe que les clusters sont proches géométriquement, ils sont bien centrés, ils ne s'éparpillent pas sur les autres. """